------------------------------------------------------------------------------------------
========================================   TODO   ========================================
------------------------------------------------------------------------------------------

Might want to get started on another aspect of this app, like zombies or ui, as a break

------------------------------------------------------------------------------------------
========================================   Code   ========================================
------------------------------------------------------------------------------------------

Event mapper is better, but not for the reason I was expecting
it's better for being event driven, but the cloud utilization is the same as allmapper

It turns out that added only occurs 1x per session (it's always the same cloud)
Updated will be that same one, and only holds as many points as are drawn on the
screen (so up to abt 200 at max, more often abt 3)

The new strat:
ditch all mapper (events are more efficient)
grab all points from added and updated in my own list (with optimizations applied)

This should all be 1 app
2 apps makes things confusing and slower

----------------------------------------------------------------------------------------------
========================================   Research   ========================================
----------------------------------------------------------------------------------------------

Optimization Research:
Gaussian Decomposition
Space Partitioning
Probablistic selection
occupancy grid map generation

GRID based processing using 3-D lidar and fusion with radar measurement
In this method, proposed by Philipp Lindner and Gerd Wanielik, laser data is processed using a multidimensional occupancy grid.[98] 
Data from a four-layer laser is pre-processed at the signal level and then processed at a higher level to extract the features of the obstacles. 
A combination two- and three-dimensional grid structure is used and the space in these structures is tessellated into several discrete cells. 
This method allows a huge amount of raw measurement data to be effectively handled by collecting it in spatial containers, the cells of the evidence grid. 
Each cell is associated with a probability measure that identifies the cell occupation. 
This probability is calculated by using the range measurement of the lidar sensor obtained over time and a new range measurement, which are related using Bayes' theorem. 
A two-dimensional grid can observe an obstacle in front of it, but cannot observe the space behind the obstacle. To address this, the unknown state behind the obstacle is assigned a probability of 0.5. 
By introducing the third dimension or in other terms using a multi-layer laser, the spatial configuration of an object could be mapped into the grid structure to a degree of complexity. 
This is achieved by transferring the measurement points into a three-dimensional grid. The grid cells which are occupied will possess a probability greater than 0.5 and the mapping would be color-coded based on the probability. 
The cells that are not occupied will possess a probability less than 0.5 and this area will usually be white space. 
This measurement is then transformed to a grid coordinate system by using the sensor position on the vehicle and the vehicle position in the world coordinate system. 
The coordinates of the sensor depend upon its location on the vehicle and the coordinates of the vehicle are computed using egomotion estimation, which is estimating the vehicle motion relative to a rigid scene. 
For this method, the grid profile must be defined. The grid cells touched by the transmitted laser beam are calculated by applying Bresenham's line algorithm.
 To obtain the spatially extended structure, a connected component analysis of these cells is performed. 
 This information is then passed on to a rotating caliper algorithm to obtain the spatial characteristics of the object. In addition to the lidar detection, 
 RADAR data obtained by using two short-range radars is integrated to get additional dynamic properties of the object, such as its velocity. The measurements are assigned to the object using a potential distance function.

Advantages and disadvantages
The geometric features of the objects are extracted efficiently, from the measurements obtained by the 3-D occupancy grid, using rotating caliper algorithm. 
Fusing the radar data to the lidar measurements give information about the dynamic properties of the obstacle such as velocity and location of the obstacle for the sensor location 
which helps the vehicle or the driver decide the action to be performed in order to ensure safety. The only concern is the computational requirement to implement this data processing technique.
 It can be implemented in real time and has been proven efficient if the 3-D occupancy grid size is considerably restricted. But this can be improved to an even wider range by using dedicated
  spatial data structures that manipulate the spatial data more effectively, for the 3-D grid representation.

Fusion of 3-D lidar and color camera for multiple object detection and tracking (this talks about some algorithms too)
The framework proposed in this method by Soonmin Hwang et al.,[99] is split into four steps. First, the data from the camera and 3-D lidar is input into the system. 
Both inputs from lidar and camera are parallelly obtained and the color image from the camera is calibrated with the lidar. To improve the efficiency, horizontal 3-D point
 sampling is applied as pre-processing. Second, the segmentation stage is where the entire 3-D points are divided into several groups per the distance from the sensor and local planes
  from close plane to far plane are sequentially estimated. The local planes are estimated using statistical analysis. The group of points closer to the sensor are used to compute the 
  initial plane. By using the current local plane, the next local plane is estimated by an iterative update. The object proposals in the 2-D image are used to separate foreground objects
   from background. For faster and accurate detection and tracking Binarized Normed Gradients (BING) for Objectness Estimation at 300fps is used.[100] BING is a combination of normed gradient and its 
   binarized version which speeds up the feature extraction and testing process, to estimate the objectness of an image window. This way the foreground and background objects are separated. 
   To form objects after estimating the objectness of an image using BING, the 3-D points are grouped or clustered. Clustering is done using DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 
   algorithm which could be robust due to its less-parametric characteristic. Using the clustered 3-D points, i.e. 3-D segment, more accurate region-of-interests (RoIs) are generated by projecting 3-D 
   points on the 2-D image.[101] The third step is detection, which is broadly divided into two parts. First is object detection in 2-D image which is achieved using Fast R-CNN[102] as this method doesn't need 
   training and it also considers an image and several regions of interest. Second is object detection in 3-D space that is done by using the spin image method.[103] This method extracts local and global 
   histograms to represent a certain object. To merge the results of 2-D image and 3-D space object detection, same 3-D region is considered and two independent classifiers from 2-D image and 3-D space are 
   applied to the considered region. Scores calibration[104] is done to get a single confidence score from both detectors. This single score is obtained in the form of probability. The final step is tracking.
    This is done by associating moving objects in present and past frame. For object tracking, segment matching is adopted. Features such as mean, standard deviation, quantized color histograms, volume size and
     number of 3-D points of a segment are computed. Euclidean distance is used to measure differences between segments. To judge the appearance and disappearance of an object, similar segments (obtained based
      on the Euclidean distance) from two different frames are taken and the physical distance and dissimilarity scores are calculated. If the scores go beyond a range for every segment in the previous frame, 
      the object being tracked is considered to have disappeared.

Should look into the process by which apple and google give these point clouds
might give some insight into my approach, how to use the points, where to edit the process.
[$] surface level arcore research

simultaneous localization and mapping
rangefinding, situational awareness
Computer stereo vision

accelerometer and gyro data for placement and rotation


I KNOW HOW TO TRACK FURNITURE!!
look for things that are elevated above the floor,
if it's above the average floor height, and the surface area is much lower than the floor height,
it's furniture!!
be sure to account for stairs, they will be their own case, maybe think about them as "Links between floors"
rather than their own thing.
I can generate my own stairs if it's not in VR

Be sure not to be confused by ceiling, maybe cull it if it's over what the person can stand on

https://www.google.com/search?q=3d+point+cloud+optimization&rlz=1C1ONGR_enUS937US937&sxsrf=AOaemvKydWY4wOExvheK_egWhwWSDAFw4w%3A1637548009523&ei=6f-aYeivH9qu0PEPqvS7kAc&ved=0ahUKEwioyKPb9ar0AhVaFzQIHSr6DnIQ4dUDCA4&uact=5&oq=3d+point+cloud+optimization&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEAgQHjoHCAAQRxCwAzoECAAQHjoGCAAQDRAeOgUIABDNAjoECCEQCkoECEEYAFCDEljLNmCDOWgEcAJ4AIABXYgBkwWSAQE5mAEAoAEByAEIwAEB&sclient=gws-wiz
https://www.google.com/search?q=3d+point+cloud&rlz=1C1ONGR_enUS937US937&sxsrf=AOaemvIolRiM7T9o0aCsOmeTJSPYXgnZlQ%3A1637539755925&ei=q9-aYdDqN4Xo9AP4sKuABw&ved=0ahUKEwiQ7NP71qr0AhUFNH0KHXjYCnAQ4dUDCA4&uact=5&oq=3d+point+cloud&gs_lcp=Cgdnd3Mtd2l6EAMyBAgjECcyBQgAEJECMgUIABCRAjIFCAAQkQIyBQgAEJECMgUIABCABDIFCAAQgAQyBQgAEIAEMgUIABCABDIFCAAQgAQ6CAgAEIAEELEDOgQIABBDOg0IABCABBCHAhCxAxAUOgsIABCABBCxAxCDAToKCAAQsQMQgwEQQzoKCC4QxwEQrwEQQzoKCAAQgAQQhwIQFEoECEEYAFAAWP0QYIYSaABwAngAgAF8iAHxCJIBBDEzLjGYAQCgAQHAAQE&sclient=gws-wiz


-------------------------------------------------------------------------------------------
=======================================    Networked Multiplayer   =================================
-------------------------------------------------------------------------------------------

"Local hosting" is on one computer
LAN - 
dig for underused ports, somebody has a resource for it
(allow for port changing too)

TCP is required for map send & recieve.
UDP may be more than I really need...

~~~new unity networking~~~
"Unity Multiplayer Networking"
(built on mlapi)
- Be insistent upon udp
? Use a grid based system of info specifics?
MLAPI = Mid level api
I probably want LLAPI if it exists... But MLAPI Might be enough 
to roll over for me.

It seems that the "Transport" API is low level
and "Netcode for GameObjects" is higher level
the benefit of not using priviledged netcode is that, as a lan game, I can just 
assume all data fed in is true. worst case scenario is that you just tell someone 
to knock off the hacking.

Ideally I would have the host broadcast on my port of choice, say 22264,
and others would check 22264 for activity, then grab the ip from the host
Although this would be 10/10 smooth, it probably won't work unless unity or android
support it...

if that doesn't work, I'll continue to a default port of my choice, say 22264,
and provide IP addresses through scanning, or a simplified number entry system
in my simplified system, I could convert ipv4s to hex, or I could map common numbers to letters

Is this a lower level api?
https://www.youtube.com/watch?v=uh8XaC0Y5MA
Apparently it's just way too much work to use??
How??

192.168.0.17
becomes
A.A.0.17
123.456.789
because it is strange, stays as
123.456.789



-------------------------------------------------------------------------------------------
=======================================    Feature List   =================================
-------------------------------------------------------------------------------------------

Blood decals
Zombies can reach under, over, through objects
    use of ik
zombies are culled by real world objects
dynamic objects will have movement detection
walls will have a zombie biotic glowing shader
Figure out game mechanics
Experiment with viewmodels
    Game should be portrait
        landscape is too unwieldly
ui
multiplayer lan code (should also share the game file)
    port choice (advanced settings)
    could even see about sharing wifi via QR


-------------------------------------------------------------------------------------------
=======================================    Tests   ========================================
-------------------------------------------------------------------------------------------

~~~~~quickupdownstarrs.11.13.21 (13mb, 337,000 pts, 0.6534375 conf)~~~~~

The good:   
is promising, shows the livingroom, stairs, and upstairs prominently

The Bad:    
there are way too many points, and a lot of duplicates.
also there seem to be some points that drift out in space...

Conclusion:
I hope that event mapper will grab fewer dupes. If not, I'll come up with a space partitioning algorithm.
I should run more analysis on the points to determine how to optimize them, starting w/ dupe elimination.

~~~~~Scans8-11.21.21 (Multiple files)~~~~~

Range tested 0.85 - 0.7
This seems to be a good range
0.85, file 8, seems to have a lot of large holes, maybe the important points are grabbed however
0.7, file 11, seems to put a lot of extra points out in space, if I'm going for furniture, I'll want
low confidence, but .7 might be too low.

-------------------------------------------------------------------------------------------
=======================================    Optimization Techniues   =================================
-------------------------------------------------------------------------------------------

Truncate or round points down 2, 3, or 4 places after the decimal, then use a 3D hashmap (Map<Float, Map<Float, Map<Float, V>>>)
spatial segmentation can optimize the ability 
Compress the number of filed points in the "processing" stage
